{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在每个时间步共享权重（即在处理各个token时使用相同的RNN）RNN能够处理不同长度的输入序列。\n",
    "\n",
    "![image](../data/image/RNN.jpg)\n",
    "\n",
    "假设有一个序列的数据，例如一段文本。将这段文本分成单词或字符，并将其作为RNN的输入。对于每一个时步，RNN会执行以下操作：\n",
    "- 接受当前时间步的输入x_t\n",
    "- 结合前一时间步的隐藏层状态h_(t-1)，计算当前时间步的隐藏层状态h_t。通常通过一个激活函数(tanh) h_t = tanh(w_hh * h_(t-1) + w_xh * x_t + b_h)\n",
    "- 基于当前时间步的隐藏层状态h_t，计算输出层y_t（RNN在时间步t输出）。 y_t = softmax(w_hy * h_t + b_y)\n",
    "\n",
    "RNN可以处理整个序列数据，并在每个时间步生成一个输出。其中w_hh、w_xh、w_hy 每个步是共享的。\n",
    "\n",
    "RNN采用BPTT反向传播算法进行训练。BPTT需要时间维度上展开RNN。从输出层一直传播到输入层。具体如下：\n",
    "- 根据模型的输出和实际标签计算损失，对每个时间步，都可以计算一个损失值，然后对所有时间步损失值进行求和，得到总损失。\n",
    "- 计算损失函数关于模型参数（权重矩阵和偏置）的梯度，使用链式求导，分别计算损失函数关于输出层、隐藏层和输入层的梯度，然后将这些沿着时间传播回去。\n",
    "- 使用优化算法（Adam、SGD）来更新模型参数。\n",
    "\n",
    "RNN局限性：\n",
    "- 训练过程中可能会遇到梯度消失和爆炸的问题。这导致网络很难学习长距离依赖关系。\n",
    "\n",
    "LSTM、GRU 广义上属于RNN，在结构上引入了门控机制，使得模型能够更好地捕捉到序列中的长距离依赖关系。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建一个简单的用于演示的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['我 喜欢 玩具', '我 爱 爸爸', '我 讨厌 挨打']\n",
    "word_list = list(set(' '.join(sentences).split()))\n",
    "word_to_idx = {word:idx for idx,word in enumerate(word_list)}\n",
    "idx_to_word = {idx:word for idx,word in enumerate(word_list)}\n",
    "voc_size = len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'玩具': 0, '挨打': 1, '爱': 2, '我': 3, '喜欢': 4, '讨厌': 5, '爸爸': 6},\n",
       " {0: '玩具', 1: '挨打', 2: '爱', 3: '我', 4: '喜欢', 5: '讨厌', 6: '爸爸'},\n",
       " 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx, idx_to_word, voc_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成NPLM训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "def make_batch():\n",
    "    #定义输入数据列表\n",
    "    intput_batch = []\n",
    "    # 定义输出数据列表\n",
    "    target_batch = []\n",
    "    #随机选取batch_size个句子\n",
    "    selected_sentences = random.sample(sentences, batch_size)\n",
    "    for sen in selected_sentences:\n",
    "        word = sen.split()\n",
    "        input = [word_to_idx[n] for n in word[:-1]]\n",
    "        output = word_to_idx[word[-1]]\n",
    "        intput_batch.append(input)\n",
    "        target_batch.append(output)\n",
    "    input_batch = torch.LongTensor(intput_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "    return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型输入数据tensor([[3, 5],\n",
      "        [3, 4]]),对应词[['我', '讨厌'], ['我', '喜欢']]\n",
      "模型输出数据torch.Size([2]),对应词['挨打', '玩具']\n"
     ]
    }
   ],
   "source": [
    "input, output = make_batch()\n",
    "input_word = []\n",
    "for input_idx in input:\n",
    "    input_word.append([idx_to_word[idx.item()] for idx in input_idx])\n",
    "print(f\"模型输入数据{input},对应词{input_word}\")\n",
    "target_word = [idx_to_word[idx.item()] for idx in output]\n",
    "print(f\"模型输出数据{output.shape},对应词{target_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义NPLM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPLM(nn.Module):\n",
    "    def __init__(self, voc_size, embedding_size, hidden):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(voc_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden, batch_first = True)\n",
    "        self.linear = nn.Linear(hidden, voc_size, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        # 只选择最后一个时间步的输出作为全连接的输入\n",
    "        output = self.linear(lstm_out[:,-1,:])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1000, cost:0.027758656069636345\n",
      "Epoch:2000, cost:0.0072835395112633705\n",
      "Epoch:3000, cost:0.0031245085410773754\n",
      "Epoch:4000, cost:0.002726348815485835\n",
      "Epoch:5000, cost:0.0008606872288510203\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5000\n",
    "model = NPLM(7, 2, 2)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "for epoch in range(epochs):\n",
    "    # 清除优化器梯度\n",
    "    optimizer.zero_grad()\n",
    "    intput_batch, target_batch = make_batch()\n",
    "    output = model(intput_batch)\n",
    "    loss = criterion(output, target_batch)\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch:{epoch+1}, cost:{loss.item()}\")\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    # 更新模型参数\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用NPLM预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '讨厌'] -> 挨打\n",
      "['我', '喜欢'] -> 玩具\n"
     ]
    }
   ],
   "source": [
    "# 要预测的提示词\n",
    "input_strs = [['我','讨厌'],['我','喜欢']]\n",
    "# 转换为对应的索引\n",
    "input_indices = [[word_to_idx[word] for word in seq] for seq in input_strs]\n",
    "# 转化为张量\n",
    "input_batch = torch.LongTensor(input_indices)\n",
    "# 计算结果，并找到概率最大的\n",
    "predict = model(input_batch).data.max(1).indices\n",
    "\n",
    "predict_str = [idx_to_word[n.item()] for n in predict]\n",
    "\n",
    "for input_seq, pred in zip(input_strs, predict_str):\n",
    "    print(input_seq,'->',pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-ENV",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
