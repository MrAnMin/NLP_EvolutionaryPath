{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bcf575a-e950-4642-8106-4a5ed2c0ac3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GPT-1\n",
    "\n",
    "![image](../data/image/GPT-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf175671-0018-47fe-90d0-83ca361dcf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from collections import Counter\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b90df2-3f53-480d-af3b-1f0246a5267c",
   "metadata": {},
   "source": [
    "### 注意力机制 & 多头注意力计算\n",
    "![image](../data/image/gpt-Attention.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af062c33-eff6-40dd-a028-e82631c8d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask):\n",
    "    '''\n",
    "    计算注意力\n",
    "    query, key, value = [batch.head,seq_len,d_k]\n",
    "    '''\n",
    "    d_k = query.size(-1)\n",
    "    # (Q K^T)/srqt(d)\n",
    "    # [batch,head,seq_len,d_k]*[batch,head,d_k, seq_len] = [batch,head,seq_len, seq_len]\n",
    "    score = torch.matmul(query, key.transpose(-1,-2)) / math.sqrt(d_k)\n",
    "    # masked_fill返回修改后的张量，masked_fill_原张量会被修改\n",
    "    # [batch,head,seq_len, seq_len]\n",
    "    score = score.masked_fill(mask, -1e9)\n",
    "    # [batch,head,seq_len, seq_len]\n",
    "    score = torch.softmax(score, dim=-1)\n",
    "    return torch.matmul(score, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b6c9f0e-d5ea-4562-a1dd-e9a2acc712d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, head):\n",
    "        '''\n",
    "        d_model:Embedding的特征维度\n",
    "        head:多头注意力中的头数\n",
    "        '''\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.head = head\n",
    "        self.d_k = d_model // head\n",
    "        self.queryLinear = nn.Linear(d_model, d_model)\n",
    "        self.keyLinear = nn.Linear(d_model, d_model)\n",
    "        self.valueLinear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.OutLinear = nn.Linear(d_model, d_model)\n",
    "        self.Layer_norm = nn.LayerNorm(normalized_shape=d_model)\n",
    "    \n",
    "    def forward(self, query, key, value, mask):\n",
    "        '''\n",
    "        query,key,value:[batch, seq_len, d_model]\n",
    "        mask:mask层\n",
    "        '''\n",
    "        batch = query.size(0)\n",
    "        query_clone = query.clone()\n",
    "        # [batch, seq_len, d_model] -> [batch,seq_len,head,d_k] -> [batch,head,seq_len,d_k]\n",
    "        query = self.queryLinear(query).view(batch, -1, self.head, self.d_k).transpose(1,2)\n",
    "        key = self.keyLinear(key).view(batch, -1, self.head, self.d_k).transpose(1,2)\n",
    "        value = self.valueLinear(value).view(batch, -1, self.head, self.d_k).transpose(1,2)\n",
    "        # [bacth,head,seq_len,d_k]\n",
    "        score = attention(query, key, value, mask)\n",
    "        score = score.transpose(1,2).contiguous().view(batch,-1, self.head * self.d_k)\n",
    "        Atten = self.OutLinear(score)\n",
    "        return self.Layer_norm(Atten + query_clone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c52df6-0baa-43ef-bf72-7e31fef94061",
   "metadata": {},
   "source": [
    "### FFN层 & 残差 归一化\n",
    "![image](../data/image/GPT-FFN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f372486-5c1b-4939-a0d5-8f378657faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_embedding, d_ff):\n",
    "        '''\n",
    "        d_embedding:embedding特征大小\n",
    "        d_ff:线性层升高维度\n",
    "        '''\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.FFN = nn.Sequential(\n",
    "            nn.Linear(d_embedding, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_embedding)\n",
    "        )\n",
    "        self.Layer_norm = nn.LayerNorm(normalized_shape=d_embedding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #残差网络\n",
    "        x_clone = x.clone()\n",
    "        ffn = self.FFN(x)\n",
    "        return self.Layer_norm(x_clone+ffn)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e947ae4-af88-4770-913f-d69c1ec6746c",
   "metadata": {},
   "source": [
    "### GPT Decoder\n",
    "在GPT中只有解码器部分,采用的mask是后续掩码\n",
    "\n",
    "![image](../data/image/GPT-DecoderLayer.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cac292f-68ad-44c6-a383-90561ec5bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_Decoder_Layer(nn.Module):\n",
    "    def __init__(self, d_embedding, head, d_ff):\n",
    "        '''\n",
    "        d_embedding:embedding的维度\n",
    "        head:多头注意力的头数\n",
    "        d_ff:前馈网络中，升高的维度\n",
    "        '''\n",
    "        super(GPT_Decoder_Layer, self).__init__()\n",
    "        self.multihead = MultiHeadAttention(d_embedding, head)\n",
    "        self.ffn = FeedForwardNetwork(d_embedding, d_ff)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        atten = self.multihead(x, x, x, mask)\n",
    "        ffn = self.ffn(atten)\n",
    "        return ffn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08b27bfe-faf8-4d3d-9de9-7032d561b4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_Decoder(nn.Module):\n",
    "    def __init__(self, n_layers, d_embedding, head, d_ff):\n",
    "        '''\n",
    "        n_layers:GPT中的解码器层数\n",
    "        d_embedding:embedding的维度\n",
    "        head:多头注意力的头数\n",
    "        d_ff:前馈网络中，升高的维度\n",
    "        '''\n",
    "        super(GPT_Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "              GPT_Decoder_Layer(d_embedding, head, d_ff) for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(self, src, src_mask):\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc7b661-8e35-43fc-a08a-74d774093232",
   "metadata": {},
   "source": [
    "### GPT_1 网络\n",
    "\n",
    "![image](../data/image/GPT_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5ee0501-af0d-4d5f-81ce-3f16d3d15f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_1(nn.Module):\n",
    "    def __init__(self, n_layers ,vocab_size, d_embedding, seq_len):\n",
    "        '''\n",
    "        n_layers:解码器的层数\n",
    "        vocab_size:数据集是总词语个数\n",
    "        d_embedding:embedding的特征长度\n",
    "        seq_len:一个序列的最大长度\n",
    "        '''\n",
    "        super(GPT_1, self).__init__()\n",
    "        # 计算词向量 [batch, seq_len] - > [batch, seq_len, d_model]\n",
    "        self.src_emb = nn.Embedding(vocab_size, d_embedding)\n",
    "        self.pos_emb = nn.Embedding(seq_len, d_embedding)\n",
    "        self.decoder = GPT_Decoder(n_layers, d_embedding, 4, 1024)\n",
    "        self.projection = nn.Linear(d_embedding, vocab_size)\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        '''\n",
    "        x = [batch,seq_len]\n",
    "        '''\n",
    "        position = torch.arange(x.size(0), device=device).unsqueeze(-1)\n",
    "        inputs_embedding = self.src_emb(x) + self.pos_emb(position)\n",
    "        # [batch.seq_len, d_model]\n",
    "        # print(inputs_embedding.shape)\n",
    "        attn_mask = create_subsequent_mask(inputs_embedding.size(1)).to(device)\n",
    "        dec_outputs = self.decoder(inputs_embedding, attn_mask)\n",
    "        # 传递给全连接层以生成预测\n",
    "        logits = self.projection(dec_outputs)\n",
    "        return logits   \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a284282-dccb-4204-8eb5-a22e0348aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(input_ids, padding_token_id):\n",
    "    '''\n",
    "    计算PAD变量\n",
    "    data:[batch, seq_len]\n",
    "    return:[batch, 1, seq_len] -> [batch, head, seq_len, seq_len]\n",
    "    '''\n",
    "    padding_mask = (input_ids == padding_token_id)\n",
    "    return padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af8bc4d9-b192-4973-84e5-5af4eff73627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subsequent_mask(seq_length):\n",
    "    '''\n",
    "    seq_length:词的长度\n",
    "    rerturn [seq_length, seq_length]\n",
    "    '''\n",
    "    # 创建后续掩码，上三角矩阵，保留对角线及以下元素，其余置为True\n",
    "    subsequent_mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool()\n",
    "    \n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa2ea516-243f-47e0-b148-655b70e3bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_subsequent_masks(input_ids, padding_token_id):\n",
    "    '''\n",
    "    input:\n",
    "        input_ids:[batch, seq_length]\n",
    "        padding_token_id:pad的tokenid\n",
    "    return:\n",
    "        batch, 1, seq_length, seq_length\n",
    "     pad的列是true时，意味着任何词对pad的注意力都是0 ,但是pad本身对其他词的注意力并不为0，所以pad行不是true\n",
    "    '''\n",
    "    seq_length = input_ids.size(1)\n",
    "    # return [batch, 1, seq_length]\n",
    "    padding_mask = create_padding_mask(input_ids, padding_token_id)\n",
    "    # rerturn [seq_length, seq_length]\n",
    "    subsequent_mask = create_subsequent_mask(seq_length)\n",
    "    combined_mask = padding_mask.unsqueeze(1) | subsequent_mask\n",
    "    return combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e76488-5e75-4101-9483-724fa9bc51c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d97033e1-e7f2-4c47-8846-6f6d772acb37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LanguageCorpus:\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        # 计算语言的最大句子长度，并加 2 以容纳特殊符号 <sos> 和 <eos>\n",
    "        self.seq_len = max([len(sentence.split()) for sentence in sentences]) + 2\n",
    "        self.word2idx = self.create_vocabulary() # 创建源语言和目标语言的词汇表\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()} # 创建索引到单词的映射\n",
    "        # 词汇表大小\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        \n",
    "    def create_vocabulary(self):\n",
    "        word2idx = {'<pad>': 0, '<sos>': 1, '<eos>': 2}\n",
    "        counter = Counter()\n",
    "        # 统计语料库的单词频率\n",
    "        for sentence in self.sentences:\n",
    "            words = sentence.split()\n",
    "            counter.update(words)\n",
    "        # 创建词汇表，并为每个单词分配一个唯一的索引\n",
    "        for word in counter:\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = len(word2idx)\n",
    "        return word2idx\n",
    "    \n",
    "    def make_batch(self, batch_size, test_batch=False):\n",
    "        input_batch, output_batch = [], [] # 初始化批数据\n",
    "        sentence_indices = torch.randperm(len(self.sentences))[:batch_size] # 随机选择句子索引\n",
    "        for index in sentence_indices:\n",
    "            sentence = self.sentences[index]\n",
    "            # 将句子转换为索引序列\n",
    "            seq = [self.word2idx['<sos>']] + [self.word2idx[word] for word in sentence.split()] + [self.word2idx['<eos>']]\n",
    "            seq += [self.word2idx['<pad>']] * (self.seq_len - len(seq)) # 对序列进行填充\n",
    "            # 将处理好的序列添加到批次中\n",
    "            input_batch.append(seq[:-1])\n",
    "            output_batch.append(seq[1:])\n",
    "        return torch.LongTensor(input_batch), torch.LongTensor(output_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "245a5be9-f67f-4db0-8096-4075d37cc208",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/traindata/lang.txt', 'r') as file:\n",
    "    sentences = [line.strip() for line in file.readlines()]\n",
    "corpus = LanguageCorpus(sentences)\n",
    "\n",
    "# vocab_size = len(corpus.vocab) # 词汇表大小\n",
    "# max_seq_len = corpus.seq_len # 最大句子长度（用于设置位置编码）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ab4bda7-5c9b-46d8-81f0-b7c90564435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # 设置设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a96ef0df-87ea-426f-84bc-ac528da7e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT_1(6,corpus.vocab_size, 512, corpus.seq_len).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "epochs = 1500\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e61c829-225f-4ac8-884d-ecdd12ce2771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0100 cost = 0.272002\n",
      "Epoch: 0200 cost = 0.204050\n",
      "Epoch: 0300 cost = 0.214156\n",
      "Epoch: 0400 cost = 0.213290\n",
      "Epoch: 0500 cost = 0.185977\n",
      "Epoch: 0600 cost = 0.211682\n",
      "Epoch: 0700 cost = 0.188645\n",
      "Epoch: 0800 cost = 0.193570\n",
      "Epoch: 0900 cost = 0.198230\n",
      "Epoch: 1000 cost = 0.197337\n",
      "Epoch: 1100 cost = 0.187307\n",
      "Epoch: 1200 cost = 0.199976\n",
      "Epoch: 1300 cost = 0.205316\n",
      "Epoch: 1400 cost = 0.185375\n",
      "Epoch: 1500 cost = 0.191484\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad() #梯度清零\n",
    "    inputs, targets = corpus.make_batch(batch_size)\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    outputs = model(inputs, device)\n",
    "    loss = criterion(outputs.view(-1, corpus.vocab_size), targets.view(-1))\n",
    "    if (epoch + 1) % 100 == 0: # 打印损失\n",
    "        print(f\"Epoch: {epoch + 1:04d} cost = {loss:.6f}\")\n",
    "    loss.backward() # 反向传播\n",
    "    optimizer.step() # 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1995449a-a2d8-40d0-97fd-0b1a016fec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试文本生成\n",
    "def generate_text(model, input_str, max_len=50):\n",
    "    model.eval()  # 将模型设置为评估（测试）模式，关闭 dropout 和 batch normalization 等训练相关的层\n",
    "    # 将输入字符串中的每个 token 转换为其在词汇表中的索引\n",
    "    input_tokens = [corpus.word2idx[token] for token in input_str]\n",
    "    # 创建一个新列表，将输入的 tokens 复制到输出 tokens 中 , 目前只有输入的词\n",
    "    output_tokens = input_tokens.copy()\n",
    "    with torch.no_grad():  # 禁用梯度计算，以节省内存并加速测试过程\n",
    "        for _ in range(max_len):  # 生成最多 max_len 个 tokens\n",
    "            # 将输出的 token 转换为 PyTorch 张量，并增加一个代表批次的维度 [1, len(output_tokens)]\n",
    "            inputs = torch.LongTensor(output_tokens).unsqueeze(0).to(device)\n",
    "            outputs = model(inputs, device) # 输出 logits 形状为 [1, len(output_tokens), vocab_size]\n",
    "            # 在最后一个维度上获取 logits 中的最大值，并返回其索引（即下一个 token）\n",
    "            _, next_token = torch.max(outputs[:, -1, :], dim=-1)            \n",
    "            next_token = next_token.item() # 将张量转换为 Python 整数            \n",
    "            if next_token == corpus.word2idx[\"<eos>\"]:\n",
    "                break # 如果生成的 token 是 EOS（结束符），则停止生成过程           \n",
    "            output_tokens.append(next_token) # 将生成的 tokens 添加到 output_tokens 列表\n",
    "            \n",
    "    # 将输出 tokens 转换回文本字符串\n",
    "    output_str = \" \".join([corpus.idx2word[token] for token in output_tokens])\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da487428-74b9-4b48-976a-666d256414df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 生成的文本 : Python libraries like Pandas and Matplotlib.\n"
     ]
    }
   ],
   "source": [
    "input_str = [\"Python\",'libraries'] # 输入一个词：Python\n",
    "generated_text = generate_text(model, input_str) # 模型跟着这个词生成后续文本\n",
    "print(\" 生成的文本 :\", generated_text) # 打印预测文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04908bc-3d8e-462b-be01-ba7b330c35a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
