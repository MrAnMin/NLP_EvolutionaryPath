{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "701ad8e3-2619-40d8-9bb0-39bca40c6c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.datasets import WikiText2 #导入 WikiText2 数据集\n",
    "from torchtext.data.utils import get_tokenizer #导入分词器\n",
    "from torchtext.vocab import build_vocab_from_iterator #导入vocabulary工具，用于从一个迭代器构建一个词汇表（Vocabulary），迭代器中包含了分词后的文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39040f14-0101-4582-bac2-83fbf951dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "max_seq_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25629aaa-05cb-494c-980a-0a59071334fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d63f74ed-58c0-4fe5-808e-f6fd83ee543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取分词器\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8d82081-583b-4002-a1df-b327166d476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_local_wikitext2(split='train'):\n",
    "    # 定义文件路径\n",
    "    file_path = f'../data/traindata/wikitext-2/wiki.{split}.tokens'\n",
    "    # 读取文件内容\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.readlines()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a0f7f23-8b40-43ce-b4e0-513754245ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = load_local_wikitext2(split='train')\n",
    "valid_iter = load_local_wikitext2(split='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abbe7793-6e8d-450a-8462-fb314b185790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表大小: 28785\n",
      "词汇示例(word to index): {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'the': 3, 'apple': 11505}\n"
     ]
    }
   ],
   "source": [
    "# 定义一个生成器函数，用于将数据集中的文本转换为tokens\n",
    "def yield_tokens(data_iter):\n",
    "    for item in data_iter:\n",
    "        yield tokenizer(item)\n",
    "\n",
    "# 创建词汇表，包括特殊tokens：\"<pad>\", \"<sos>\", \"<eos>\"\n",
    "#specials:一个包含特殊符号的列表，如<pad>（填充符），<unk>（未知词标记）等。这些特殊符号会被添加到词汇表的开始位置。\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "# 设置当查询的词汇项不在词汇表中时返回的默认索引值。当查询的词汇不在词汇表中默认返回<pad>的值\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "'''\n",
    "vocab的几个方法：\n",
    "__getitem__(self, token): 返回给定词汇项的索引。\n",
    "__len__(self): 返回词汇表中词汇项的数量。\n",
    "get_itos(self): 返回一个列表，其中包含词汇表中所有词汇项，索引即为它们在词汇表中的位置。\n",
    "get_stoi(self): 返回一个字典，键为词汇项，值为它们在词汇表中的索引。\n",
    "set_default_index(self, index): 设置当查询的词汇项不在词汇表中时返回的默认索引值。\n",
    "'''\n",
    "\n",
    "# 打印词汇表信息\n",
    "print(\"词汇表大小:\", len(vocab))\n",
    "print(\"词汇示例(word to index):\", {word: vocab[word] for word in [\"<pad>\", \"<sos>\", \"<eos>\", \"the\", \"apple\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68a3af5e-ffdf-4cb0-a24e-6deb0d1fed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, data_iter, vocab, max_len=max_seq_len):\n",
    "        super(WikiDataset, self).__init__()\n",
    "        self.data = []\n",
    "        for sentence in data_iter:\n",
    "            tokens = tokenizer(sentence)[:max_len-2]\n",
    "            tokens = [vocab['<sos>']] + vocab(tokens) + [vocab['<eos>']]\n",
    "            self.data.append(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = self.data[idx][:-1]\n",
    "        target = self.data[idx][1:]\n",
    "        return torch.LongTensor(source), torch.LongTensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f1a0f18-fdf4-4a71-bdc9-9cc526d840d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask):\n",
    "    '''\n",
    "    query,key,value:[batch,head,seq_len,d_k]\n",
    "    '''\n",
    "    d_k = query.size(-1)\n",
    "    # [batch,head,seq_len,d_k]*[batch,head,d_k,seq_len]=[batch,head,d_k,seq_len,seq_len]\n",
    "    # QK^T / srqt(d_k)\n",
    "    score = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    score = score.masked_fill_(mask, -1e9)\n",
    "    score = torch.softmax(score, dim=-1)\n",
    "    return torch.matmul(score, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92a8f85b-912c-4b91-897a-c8161cdb284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head, d_embedding):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.head = head\n",
    "        self.d_k = d_embedding // head\n",
    "        self.queryLinear = nn.Linear(d_embedding, d_embedding)\n",
    "        self.keyLinear = nn.Linear(d_embedding, d_embedding)\n",
    "        self.valueLinear = nn.Linear(d_embedding, d_embedding)\n",
    "        self.fc_out = nn.Linear(d_embedding, d_embedding)\n",
    "        \n",
    "        self.lary_norm = nn.LayerNorm(normalized_shape=d_embedding)\n",
    "        \n",
    "    def forward(self, query, key, value, att_mask):\n",
    "        '''\n",
    "        query, key, value：【batch,seq_len, d_embedding】\n",
    "        '''\n",
    "        batch = query.size(0)\n",
    "        query_clone = query.clone()\n",
    "        # [batch,seq_len,d_embedding]->[batch,seq_len,head,d_k]->[batch,head,seq_len,d_k]\n",
    "        query = self.queryLinear(query).view(batch,-1,self.head, self.d_k).transpose(1,2)\n",
    "        key = self.queryLinear(key).view(batch,-1,self.head, self.d_k).transpose(1,2)\n",
    "        value = self.queryLinear(value).view(batch,-1,self.head, self.d_k).transpose(1,2)\n",
    "        atten = attention(query, key, value, att_mask)\n",
    "        #[batch,head,seq_len,d_k]->[batch,seq_len,head,d_k]-> [batch,seq_len,d_embedding]\n",
    "        atten = atten.transpose(1,2).contiguous().view(batch,-1,self.head*self.d_k)\n",
    "        Atten = self.fc_out(atten)\n",
    "        return self.lary_norm(Atten + query_clone) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f8e6c1-06ca-4ccc-8009-e4c47656c5df",
   "metadata": {},
   "source": [
    "![image](../data/image/GPT-FFN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc286fe9-66b3-462d-bbf3-9ca6175af572",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_embedding, d_ff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.ForwardNetwork = nn.Sequential(\n",
    "            nn.Linear(d_embedding, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_embedding)\n",
    "        )\n",
    "        self.lary_norm = nn.LayerNorm(normalized_shape=d_embedding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_clone = x.clone()\n",
    "        ForwardNetwork = self.ForwardNetwork(x)\n",
    "        return self.lary_norm(x_clone + ForwardNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d80148-c8ef-46c4-bb58-1f3fcd9d2adf",
   "metadata": {},
   "source": [
    "![image](../data/image/GPT-DecoderLayer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "966b6c83-c0e8-4936-8a2a-c264fa85114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_embedding, head, d_ff):\n",
    "        super(GPT_DecoderLayer, self).__init__()\n",
    "        self.Multi = MultiHeadAttention(head, d_embedding)\n",
    "        self.ffn = FeedForwardNetwork(d_embedding, d_ff)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        atten  = self.Multi(x, x, x, mask)\n",
    "        ffn = self.ffn(atten)\n",
    "        return ffn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61f435a7-9445-4641-9538-f68190dac9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_Decoders(nn.Module):\n",
    "    def __init__(self, n_layers,d_embedding, head,  d_ff):\n",
    "        # GPT_Decoders(n_layers, d_embedding, 4, 1024)\n",
    "        super(GPT_Decoders, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                GPT_DecoderLayer(d_embedding, head, d_ff) for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67a41066-1b02-47ef-b0e0-2be538d371e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subsequent_mask(seq_length):\n",
    "    '''\n",
    "    seq_length:词的长度\n",
    "    rerturn [seq_length, seq_length]\n",
    "    '''\n",
    "    # 创建后续掩码，上三角矩阵，保留对角线及以下元素，其余置为True\n",
    "    subsequent_mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool()\n",
    "    \n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b2ee67f-645f-4b07-be37-59c6ca5b5a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_1(nn.Module):\n",
    "    def __init__(self, n_layers ,vocab_size, d_embedding, seq_len):\n",
    "        # model = GPT_1(6, len(vocab), 512, max_seq_len).to(device)\n",
    "        super(GPT_1, self).__init__()\n",
    "        self.src_emb = nn.Embedding(vocab_size, d_embedding)\n",
    "        self.pos_emb = nn.Embedding(seq_len, d_embedding)\n",
    "        self.decoder = GPT_Decoders(n_layers, d_embedding, 4, 1024)\n",
    "        self.projection = nn.Linear(d_embedding, vocab_size)\n",
    "        \n",
    "    def forward(self, x, device):\n",
    "        position = torch.arange(x.size(0), device=device).unsqueeze(-1)\n",
    "        inputs_embedding = self.src_emb(x) + self.pos_emb(position)\n",
    "        # [batch.seq_len, d_model]\n",
    "        attn_mask = create_subsequent_mask(inputs_embedding.size(1)).to(device)\n",
    "        dec_outputs = self.decoder(inputs_embedding, attn_mask)\n",
    "        # 传递给全连接层以生成预测\n",
    "        logits = self.projection(dec_outputs)\n",
    "        return logits   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a320b357-18af-4104-bc53-b5c42ee60185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequences, padding_value=0, length=None):\n",
    "    '''\n",
    "    进行批量训练时，将序列长度补齐\n",
    "    '''\n",
    "    max_length = max(len(seq) for seq in sequences) if length is None else length\n",
    "    # 创建一个具有适当形状的全零张量，用于存储补齐后的序列\n",
    "    result = torch.full((len(sequences), max_length), padding_value, dtype=torch.long)\n",
    "    # 遍历序列，将每个序列的内容复制到结果张量中\n",
    "    for i, seq in enumerate(sequences):\n",
    "        end = len(seq)\n",
    "        result[i, :end] = seq[:end]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5001abaa-e0ad-4739-99ad-93dc565aee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义collate_fn函数，用于将一个批次的数据整理成适当的形状\n",
    "def collate_fn(batch):\n",
    "    # 从批次中分离源序列和目标序列\n",
    "    sources, targets = zip(*batch)    \n",
    "    # 计算批次中的最大序列长度\n",
    "    max_length = max(max(len(s) for s in sources), max(len(t) for t in targets))    \n",
    "    # 使用pad_sequence函数补齐源序列和目标序列\n",
    "    sources = pad_sequence(sources, padding_value=vocab[\"<pad>\"], length=max_length)\n",
    "    targets = pad_sequence(targets, padding_value=vocab[\"<pad>\"], length=max_length)    \n",
    "    # 返回补齐后的源序列和目标序列\n",
    "    return sources, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df14179e-a86f-42f9-b11e-7e3a63c6bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WikiDataset(train_iter, vocab) # 创建训练数据集\n",
    "valid_dataset = WikiDataset(valid_iter, vocab) # 创建验证数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72c63d2f-3cfd-4edc-beb1-dba4f5eda0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "378953d3-fc22-4ed5-b04f-057f7ec04cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a172cbbc-026d-49ad-8a7a-061f6c1c1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = GPT_1(6, len(vocab), 512, max_seq_len).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # 优化器\n",
    "epochs = 2  # 训练轮次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "861a52cf-fadf-4273-bb98-bac55c3b0880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/12240, Loss: 7.0839715003967285\n",
      "Batch 1000/12240, Loss: 5.8813700675964355\n",
      "Batch 1500/12240, Loss: 6.6938605308532715\n",
      "Batch 2000/12240, Loss: 6.501121520996094\n",
      "Batch 2500/12240, Loss: 6.339188575744629\n",
      "Batch 3000/12240, Loss: 5.825780391693115\n",
      "Batch 3500/12240, Loss: 5.920065402984619\n",
      "Batch 4000/12240, Loss: 6.576527118682861\n",
      "Batch 4500/12240, Loss: 5.715553283691406\n",
      "Batch 5000/12240, Loss: 6.186413764953613\n",
      "Batch 5500/12240, Loss: 5.766402721405029\n",
      "Batch 6000/12240, Loss: 6.603525638580322\n",
      "Batch 6500/12240, Loss: 5.811763763427734\n",
      "Batch 7000/12240, Loss: 6.217658996582031\n",
      "Batch 7500/12240, Loss: 5.253559112548828\n",
      "Batch 8000/12240, Loss: 0.3145367205142975\n",
      "Batch 8500/12240, Loss: 6.071967124938965\n",
      "Batch 9000/12240, Loss: 5.8089599609375\n",
      "Batch 9500/12240, Loss: 6.809699535369873\n",
      "Batch 10000/12240, Loss: 2.0836989879608154\n",
      "Batch 10500/12240, Loss: 5.400591850280762\n",
      "Batch 11000/12240, Loss: 5.6525492668151855\n",
      "Batch 11500/12240, Loss: 5.002734184265137\n",
      "Batch 12000/12240, Loss: 4.530876159667969\n",
      "Epoch 1/2, Average Loss: 5.414142775417495\n",
      "Batch 500/12240, Loss: 5.5879058837890625\n",
      "Batch 1000/12240, Loss: 6.48300838470459\n",
      "Batch 1500/12240, Loss: 5.481262683868408\n",
      "Batch 2000/12240, Loss: 5.937962055206299\n",
      "Batch 2500/12240, Loss: 1.452423334121704\n",
      "Batch 3000/12240, Loss: 5.443799018859863\n",
      "Batch 3500/12240, Loss: 5.599474906921387\n",
      "Batch 4000/12240, Loss: 5.526385307312012\n",
      "Batch 4500/12240, Loss: 5.725088596343994\n",
      "Batch 5000/12240, Loss: 5.58143949508667\n",
      "Batch 5500/12240, Loss: 5.684781551361084\n",
      "Batch 6000/12240, Loss: 5.455925941467285\n",
      "Batch 6500/12240, Loss: 5.182502269744873\n",
      "Batch 7000/12240, Loss: 5.7879462242126465\n",
      "Batch 7500/12240, Loss: 5.601396560668945\n",
      "Batch 8000/12240, Loss: 5.238949298858643\n",
      "Batch 8500/12240, Loss: 5.358638286590576\n",
      "Batch 9000/12240, Loss: 5.276968002319336\n",
      "Batch 9500/12240, Loss: 4.459814548492432\n",
      "Batch 10000/12240, Loss: 5.236854553222656\n",
      "Batch 10500/12240, Loss: 5.731582164764404\n",
      "Batch 11000/12240, Loss: 5.697476387023926\n",
      "Batch 11500/12240, Loss: 5.779514789581299\n",
      "Batch 12000/12240, Loss: 0.1458037942647934\n",
      "Epoch 2/2, Average Loss: 4.880122681287135\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (source, target) in enumerate(train_dataloader): # 用Dataloader加载数据\n",
    "        inputs, targets = source.to(device), target.to(device)\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "        outputs = model(inputs, device)  # 获取模型输出\n",
    "        loss = criterion(outputs.view(-1, len(vocab)), targets.view(-1))  # 计算损失\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新参数\n",
    "        epoch_loss += loss.item()        \n",
    "        if (batch_idx + 1) % 500 == 0: # 每500个批次打印一次损失\n",
    "            print(f\"Batch {batch_idx + 1}/{len(train_dataloader)}, Loss: {loss.item()}\")    \n",
    "    epoch_loss /= len(train_dataloader) # 每轮打印一次损失\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "770410e8-3aa7-485e-898c-2ba131479789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_beam_search(model, input_str, device,max_len=50, beam_width=5):\n",
    "    model.eval()  # 将模型设置为评估（测试）模式，关闭dropout和batch normalization等训练相关的层\n",
    "    # 将输入字符串中的每个token 转换为其在词汇表中的索引\n",
    "    input_tokens = [vocab[token] for token in input_str.split()]\n",
    "    # 创建一个列表，用于存储候选序列\n",
    "    candidates = [(input_tokens, 0.0)]\n",
    "    with torch.no_grad():  # 禁用梯度计算，以节省内存并加速测试过程\n",
    "        for _ in range(max_len):  # 生成最多max_len个tokens\n",
    "            new_candidates = []\n",
    "            for candidate, candidate_score in candidates:\n",
    "                inputs = torch.LongTensor(candidate).unsqueeze(0).to(device)\n",
    "                outputs = model(inputs,device) # 输出 logits形状为[1, len(output_tokens), vocab_size]\n",
    "                logits = outputs[:, -1, :] # 只关心最后一个时间步（即最新生成的token）的logits\n",
    "                # 找到具有最高分数的前beam_width个tokens\n",
    "                scores, next_tokens = torch.topk(logits, beam_width, dim=-1)\n",
    "                final_results = [] # 初始化输出序列\n",
    "                for score, next_token in zip(scores.squeeze(), next_tokens.squeeze()):\n",
    "                    new_candidate = candidate + [next_token.item()]\n",
    "                    new_score = candidate_score - score.item()  # 使用负数，因为我们需要降序排列\n",
    "                    if next_token.item() == vocab[\"<eos>\"]:\n",
    "                        # 如果生成的token是EOS（结束符），将其添加到最终结果中\n",
    "                        final_results.append((new_candidate, new_score))\n",
    "                    else:\n",
    "                        # 将新生成的候选序列添加到新候选列表中\n",
    "                        new_candidates.append((new_candidate, new_score))\n",
    "            # 从新候选列表中选择得分最高的beam_width个序列\n",
    "            candidates = sorted(new_candidates, key=lambda x: x[1])[:beam_width]\n",
    "    # 选择得分最高的候选序列\n",
    "    best_candidate, _ = sorted(candidates, key=lambda x: x[1])[0]\n",
    "    # 将输出 token 转换回文本字符串\n",
    "    output_str = \" \".join([vocab.get_itos()[token] for token in best_candidate if vocab.get_itos()[token] != \"<pad>\"])\n",
    "    return output_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f657a47b-b6a0-4433-b6a0-b33503ca7544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的文本: my name <unk> ( <unk> ) , <unk> ( <unk> ( <unk> ) , <unk> ( <unk> ) , <unk> ( <unk> ( <unk> ) , <unk> ( <unk> ( <unk> ) , <unk> ( <unk> ( <unk> ) , <unk> ( <unk> ( <unk> ) , <unk> ( <unk> ) ,\n"
     ]
    }
   ],
   "source": [
    "input_str = \"my name\"  # 输入几个词\n",
    "generated_text = generate_text_beam_search(model, input_str, device)  # 模型跟着这些词生成后续文本\n",
    "print(\"生成的文本:\", generated_text)  # 打印生成的文本"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
